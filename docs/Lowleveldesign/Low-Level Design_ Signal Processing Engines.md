### Low-Level Design: Signal Processing Engines \#\#\#\# 1.0 Introduction \#\#\#\#\# 1.1. Purpose and Scope The Signal Processing Engines are the analytical core of the Signal Factory observability platform. They transform the firehose of low-level, per-record Evidence produced by the enforcement layer into high-level, actionable signals about the health, performance, and compliance of our data ecosystems. By aggregating, thresholding, and correlating these immutable facts, the engines provide the foundational insights that power everything from automated incident response to deterministic root cause analysis. This document provides the detailed, low-level, and implementable design for the Signal Processing Engines component. It defines the specific responsibilities, data contracts, storage models, and operational requirements necessary for a platform engineering team to build, deploy, and operate this system. The scope of this design is strictly defined to ensure clarity and independent implementation: | In Scope | Out of Scope | | \------ | \------ | | Detailed design of Volume, Freshness, Contract Compliance, and PII Signal Engines | The design of the Policy Enforcer component | | The Evidence (input) and Signal (output) data contracts | The Gateway Control Plane APIs that manage configuration | | Data storage models and schemas for operational state in DynamoDB and the causal graph in Neptune | The internal implementation details of the Central Streaming Platform | | Deployment architecture on EKS, scaling strategies, and core operational requirements (observability, failure modes) | The reasoning engine or natural language processing components of the RCA Copilot | The engines' design is fundamentally shaped by the platform's "Pattern 1" out-of-band architecture, which dictates how they consume data and interact with the broader ecosystem. \#\#\#\#\# 1.2. Architectural Context The Signal Processing Engines operate exclusively within the "Pattern 1: Out-of-Band Enforcement" architecture. This means they are not on the critical write path of producer services; instead, they consume data *after* it has been published to the Central Streaming Platform. This architectural choice deliberately prioritizes the stability and performance of the central platform while still enabling pre-consumption safety and comprehensive observability for all downstream data consumers. The data flow that governs the engines' operation is simple and decoupled: 1\. **Producers** (e.g., Orders Service) publish raw business events to their designated topics on the **Central Streaming Platform** . 2\. The Policy Enforcer service consumes these raw events in near-real-time. 3\. For each raw event, the Policy Enforcer runs a series of validation gates and produces a single, immutable Evidence event to the central Evidence Bus (a dedicated Kafka topic). This Evidence captures the ground truth of the record's validity at that point in time. 4\. The Signal Processing Engines are independent consumers of the Evidence Bus, processing these facts to compute higher-level health signals. This decoupled flow is governed by a set of core design principles that ensure the system's long-term maintainability, scalability, and correctness. \#\#\#\#\# 1.3. Core Design Principles Establishing clear design principles is critical for building a platform that can evolve without architectural drift or ambiguity in ownership. The Signal Processing Engines are designed and must be implemented according to the following tenets: \* **Strict Separation of Concerns:** There is a fundamental rule separating the enforcement and signal layers. The Policy Enforcer is solely responsible for establishing per-record truth (e.g., "this record fails its contract"). The Signal Engines are solely responsible for establishing system-level health (e.g., "the contract compliance rate for this dataset has breached its SLO"). This document will demonstrate how this principle is upheld through strict interface contracts. \* **Evidence is the Immutable Contract:** The signal\_factory.evidence Kafka topic is the sole, non-negotiable API and source of truth for all Signal Engines. Engines MUST NOT consume raw business topics directly or attempt to re-validate payloads. They operate exclusively on the immutable facts presented in the Evidence stream. \* **Configuration over Code:** All business logic, thresholds, SLOs, and alerting policies must be managed by the Gateway Control Plane and consumed by the engines as dynamic configuration. Logic such as "a Tier-1 dataset's freshness SLO is 2 minutes" must never be hardcoded into the engine's implementation. \* **Trace-Anchored Causality:** To enable deterministic root cause analysis, all outputs—from computed signals to generated incidents—must preserve the trace\_id from the source Evidence whenever it is available. This ensures an unbroken causal chain from a producer's action to an SRE's alert, which is the core value proposition of the RCA Copilot. These principles are formalized into specific component responsibilities and data contracts in the following sections. \#\#\#\# 2.0 Core Responsibilities and Interfaces \#\#\#\#\# 2.1. Delineation of Responsibilities A clear and unambiguous definition of ownership is the most critical factor in preventing logic duplication, operational friction, and architectural drift between the enforcement and signal processing layers. The following matrix defines the hard boundaries between the Policy Enforcer and the Signal Processing Engines. **Responsibility Matrix** | Signal Processing Engines | Policy Enforcer | | \------ | \------ | | **Owns:**

* Aggregation of Evidence over time windows  
* Computation of rates, ratios, and trends (e.g., compliance rate)  
* Static thresholding against SLOs  
* Dynamic anomaly detection  
* Incident creation and severity classification  
* Cross-signal correlation for RCA context

**Does NOT Own:**

* Schema validation logic  
* Contract evaluation logic  
* PII detection rules  
* Parsing of raw business events  
* Writing records to quarantine

| **Owns:**

* Parsing and canonicalization of raw events  
* Per-record schema validation  
* Per-record contract validation (required fields, constraints)  
* Per-record PII detection  
* Emission of PASS/FAIL Evidence for every record  
* Producer attribution and identity confidence scoring

**Does NOT Own:**

* Incident creation logic  
* SLO breach decisions  
* Anomaly detection models  
* Alerting thresholds  
* Aggregation logic over time windows

| These responsibilities are enforced through two strictly defined data contracts: the input interface (EvidenceEvent) and the output interface (SignalEvent). \#\#\#\#\# 2.2. The Evidence Contract (Input Interface) The EvidenceEvent is the formal API contract that decouples the Signal Engines from the Policy Enforcer. It represents the immutable, per-record ground truth produced by the enforcement layer. All Signal Engines MUST consume events conforming to this schema from the signal\_factory.evidence topic. Adherence to this schema is mandatory. The EvidenceEvent schema is defined as follows: | Field | Type | Description | Required? | | \------ | \------ | \------ | \------ | | evidence\_id | String | A unique identifier for this piece of evidence (e.g., a UUID). | Yes | | timestamp | String | ISO-8601 timestamp indicating when the evidence was generated. | Yes | | stage | String | The stage of observation (e.g., OBSERVED\_RAW in Pattern 1). | Yes | | dataset\_urn | String | The unique, canonical identifier for the dataset. | Yes | | producer | Object | An object containing information about the producer. | Yes | | producer.producer\_id | String | The identified producer service. | Yes | | producer.identity\_confidence | String | Confidence score (HIGH, MEDIUM, LOW, UNKNOWN). | Yes | | source | Object | Details about the source of the raw record. | Yes | | source.raw\_topic | String | The Kafka topic where the raw message was observed. | Yes | | source.partition | Integer | The Kafka partition of the raw message. | Yes | | source.offset | Long | The Kafka offset of the raw message. | Yes | | validation | Object | The result of the gate pipeline evaluation. | Yes | | validation.result | String | The final outcome: PASS or FAIL. | Yes | | validation.failed\_gates | Array | A list of gates that failed (e.g., "CONTRACT", "PII"). | Yes | | validation.reason\_codes | Array | Machine-readable codes explaining the failure. | Yes | | otel | Object | OpenTelemetry context. While optional, its presence is critical for Trace-Anchored Causality. | No | | otel.trace\_id | String | The trace\_id from the producer's request, if available. | No | | otel.span\_id | String | The span\_id of the Policy Enforcer's evaluation span. | No | | contract | Object | Optional details about the contract applied. | No | | schema | Object | Optional details about the schema applied. | No | | pii | Object | Optional details about detected PII. | No | Just as the input to every engine is standardized through this contract, so too is the output. \#\#\#\#\# 2.3. The Signal Contract (Output Interface) To ensure downstream consistency for incident management, dashboarding, and the RCA Copilot, all Signal Engines MUST produce a standardized SignalEvent. This event represents a computed observation about the health of a dataset over a specific time window. The SignalEvent schema is defined as follows: | Field | Type | Description | Required? | | \------ | \------ | \------ | \------ | | signal\_id | String | A unique identifier for this signal computation instance. | Yes | | signal\_type | String | The type of signal (e.g., VOLUME, FRESHNESS, CONTRACT\_COMPLIANCE, PII). | Yes | | dataset\_urn | String | The dataset this signal applies to. | Yes | | severity | String | The assessed severity (NONE, SEV-1, SEV-2, etc.). If NONE, no incident is implied. | Yes | | window | Object | The time window over which the signal was computed, with start and end timestamps. | Yes | | computed\_metrics | Object | A key-value map of the specific metrics calculated (e.g., {"compliance\_rate": 0.98}). | Yes | | evidence\_refs | Array | A list of sample evidence\_ids that contributed to this signal, used for RCA drill-down. | Yes | | runbook\_ref | String | A reference to the operational runbook for this signal type and dataset. | No | | producer\_id | String | The producer ID, if the signal is scoped to a single producer. | No | | trace\_refs | Array | A list of relevant trace\_ids associated with the incident. | No | The following sections describe how these components and contracts are realized in a running system. \#\#\#\# 3.0 System-Level Architecture \#\#\#\#\# 3.1. Data Flow and Processing The signal processing subsystem is designed around a classic fan-in/fan-out architectural pattern. The Policy Enforcer acts as the "fan-in" point, consuming from multiple raw data topics and consolidating its validation outcomes into a single, canonical Evidence Bus. From there, the system "fans out" to multiple, independent Signal Processing Engines. Each engine, or logical group of engines, subscribes to the Evidence Bus and processes the same stream of facts in parallel to compute its specific type of health signal. This decoupled design allows for independent scaling, deployment, and development of each signal type. The end-to-end sequence from a producer's action to an RCA insight is illustrated below: mermaid sequenceDiagram autonumber participant OS as Orders Service participant CSP as Central Streaming Platform participant PE as Policy Enforcer participant EB as Evidence Bus (Kafka) participant SE as Signal Engines participant INC as Incident Mgmt participant RCA as RCA Copilot OS-\>\>CSP: Publish raw business event PE-\>\>CSP: Consume raw event PE-\>\>PE: Evaluate gates (schema, contract, etc.) PE-\>\>EB: Emit Evidence (PASS/FAIL) SE-\>\>EB: Consume Evidence SE-\>\>SE: Compute signals (Volume, Freshness, etc.) alt SLO breached SE-\>\>INC: Create Incident end INC--\>\>RCA: Trigger RCA with evidence references RCA-\>\>EB: Fetch evidence for analysis RCA--\>\>INC: Provide deterministic explanation  This flow is supported by a dual-database strategy designed for both operational performance and deep analytical capability. \#\#\#\#\# 3.2. Data Storage Model To optimize for both real-time operational needs and complex analytical queries, the Signal Factory employs a dual-database strategy. **DynamoDB** is used for its high-performance key-value access, serving as the system's operational state store for fast lookups and dashboards. In parallel, **Amazon Neptune** is used as a graph database to store the causal relationships between entities, enabling the deep traversal queries required for deterministic root cause analysis. \#\#\#\#\#\# 3.2.1 DynamoDB: Operational State The DynamoDB model provides fast, authoritative configuration lookups for the Policy Enforcer and stores the operational state necessary for the Signal Engines. An LLD for this system is non-implementable without defining all core configuration and state tables. **Table:** **DatasetRegistry** | Aspect | Description | | \------ | \------ | | **Purpose** | To provide an authoritative runtime lookup for dataset configuration. This is a hot-path dependency for the Policy Enforcer and a configuration source for Signal Engines (e.g., for SLOs). | | **Primary Key** | PK: dataset\_urnSK: METADATA | | **Key Attributes** | tier, schema\_id, contract\_id, pii\_policy, expected\_rpm, freshness\_sla\_sec, owners. | **Table:** **DatasetResolutionMap** | Aspect | Description | | \------ | \------ | | **Purpose** | To resolve raw platform input (e.g., a Kafka topic name) to a canonical dataset\_urn. This allows the Policy Enforcer to apply the correct policies to an incoming raw record. | | **Primary Key** | PK: source\_type (e.g., TOPIC)SK: source\_value (e.g., raw.orders.events) | | **Key Attributes** | dataset\_urn, resolution\_confidence, extraction\_rule. | Signal Engines are responsible for writing to and maintaining state in two primary DynamoDB tables: SignalState and IncidentIndex. **Table:** **SignalState** | Aspect | Description | | \------ | \------ | | **Purpose** | To store the most recent computed value and health status for each signal on a per-dataset basis. This powers real-time dashboards and allows for alert suppression. | | **Primary Key** | PK: dataset\_urnSK: signal\_type | | **Key Attributes** | current\_value (e.g., 0.995), severity (SEV-1, NONE), status (OPEN, CLOSED), last\_evaluated (timestamp), incident\_id. | **Table:** **IncidentIndex** | Aspect | Description | | \------ | \------ | | **Purpose** | To provide a fast, queryable index of active and recently resolved incidents. This is the primary source for incident list views and SRE workflows. | | **Primary Key** | PK: incident\_idSK: METADATA | | **Key Attributes** | dataset\_urn, signal\_type, severity, started\_at, status, primary\_trace\_id. | \#\#\#\#\#\# 3.2.2 Neptune: Causal Graph Signal Engines are responsible for populating the Neptune graph with the relationships that enable the RCA Copilot to trace failures back to their source. The core relationships created by the engines include: \* (Incident)-:TRIGGERED\_BY-\>(Signal): Links an incident to the specific signal that breached its threshold. \* (Signal)-:DERIVED\_FROM-\>(Evidence): Connects a signal computation to the sample of evidence records that informed it. \* (Signal)-:APPLIES\_TO-\>(Dataset): Associates a signal with the dataset it is monitoring. To manage costs and maintain query performance, engines MUST adhere to strict rules about what not to write to Neptune: \* **Do not write every** **Evidence** **record.** Only write FAIL evidence or a small, statistically significant sample of PASS evidence. \* **Do not write time-series data.** Metrics like per-minute volume belong in a dedicated metrics backend (New Relic), not the graph. \* **Do not write high-cardinality raw identifiers as nodes.** Fields like order\_id or customer\_id should be properties on an Evidence node, not nodes themselves. \* **Use the "Failure Signature" optimization.** Instead of creating millions of nodes for identical contract failures, create a single FailureSignature node (e.g., for MISSING\_FIELD:customer\_id on the orders dataset) and link individual Evidence records to it. We now transition from *where* data is stored to *how* the services that process it are deployed and scaled. \#\#\#\#\# 3.3. Deployment and Scaling The operational goals for the Signal Engines are high availability, horizontal scalability, and strict resource isolation to prevent "noisy neighbor" problems. The deployment model is the direct, physical implementation of the Strict Separation of Concerns principle, ensuring both logical and resource isolation between different signal computations. The standard deployment model is on **Amazon EKS** . Each Signal Engine (or a logical group of related engines, such as "compliance signals") should be packaged and run as a separate Kubernetes Deployment. This ensures that a failure or resource issue in one engine does not affect the others. Scaling will be managed automatically and dynamically. Every Signal Engine Deployment MUST be governed by a HorizontalPodAutoscaler (HPA). The HPA should be configured to scale the number of pods based on two primary metrics: 1\. **CPU Utilization:** To handle computational load during signal processing. 2\. **Kafka Consumer Lag:** To ensure the engine can keep up with the volume of Evidence being produced, preventing signal staleness. To further enforce isolation and prevent a single slow consumer from impacting all signals, a dedicated Kafka consumer group must be used for each signal type. This strategy avoids noisy-neighbor effects at the consumption layer. For example: \* sf-signal-volume \* sf-signal-freshness \* sf-signal-pii \* sf-signal-contract With the high-level architecture defined, we can now drill into the specific logic of each individual engine. \#\#\#\# 4.0 Detailed Signal Engine Designs The following subsections provide the specific, implementable logic for the four core signal types. Each design is self-contained and adheres to the principles and contracts established in the preceding sections. \#\#\#\#\# 4.1. Volume Signal \* **Purpose:** To detect when the rate of valid events for a dataset significantly deviates from an expected baseline, indicating potential upstream outages or producer-side issues. \* **Inputs:** Evidence events from the signal\_factory.evidence topic, specifically counting events where validation.result is PASS. The engine also consumes a dataset policy defining the expected rate and thresholds. \* **Computation Logic:** The engine uses a tumbling time window (e.g., 1 minute) to count the number of PASS Evidence events for each dataset\_urn. It computes the observed\_rpm (records per minute) and compares it to the expected\_rpm defined in the policy to calculate a drop\_pct deviation. \* **Incident Threshold Policy:** Incidents are created based on the severity of the deviation and the tier of the dataset. | Tier | Condition | Sustain Duration | Severity | | \------ | \------ | \------ | \------ | | Tier-1 | drop\_pct \>= 40% | 3 minutes | SEV-1 | | Tier-1 | drop\_pct \>= 20% | 5 minutes | SEV-2 | | Tier-2 | drop\_pct \>= 50% | 10 minutes | SEV-2 | | Any | spike\_pct \>= 100% | 5 minutes | SEV-2 | \* **Incident Payload Schema:** \* **drop\_pct** : Provides the responding SRE with the immediate magnitude of the outage (e.g., 90% drop). \* **observed\_rpm** **vs.** **expected\_rpm** : Gives absolute numbers for context, distinguishing between a drop from 1M to 100k vs. 100 to 10\. \* **evidence\_sample** : Offers direct links to the last-known-good records, providing a starting point for investigation. \#\#\#\#\# 4.2. Freshness Signal \* **Purpose:** To detect when a dataset has not produced a recent, valid event within its defined Service Level Agreement (SLA), indicating that the data is stale. \* **Inputs:** The timestamp and validation.result from every Evidence event, and the freshness\_sla\_seconds from the dataset's policy. \* **Computation Logic:** The engine maintains the timestamp of the last seen PASS evidence (last\_pass\_ts) and the last seen evidence of any kind (last\_any\_ts) for each dataset. It periodically calculates freshness\_lag\_pass \= now \- last\_pass\_ts and freshness\_lag\_any \= now \- last\_any\_ts. This distinction helps differentiate between a silent producer and a producer that is only sending failing data. \* **Incident Threshold Policy:** Breaches are determined by comparing the computed lag against the dataset's tier-based SLA. | Condition | Meaning | Tier-1 SLA | Severity | | \------ | \------ | \------ | \------ | | freshness\_lag\_any \> SLA | No events are arriving at all | 120s | SEV-1 | | freshness\_lag\_pass \> SLA AND freshness\_lag\_any \<= SLA | Events are arriving but are all failing validation | 120s | SEV-1 | | freshness\_lag\_pass \> 2 \* SLA | The freshness breach is prolonged | 120s | Escalation | \* **Incident Payload Schema:** \* **freshness\_lag\_pass\_seconds** **vs** **freshness\_lag\_any\_seconds** : Immediately tells the SRE whether the upstream producer is silent (both values are high) or actively sending bad data (only pass lag is high). \* **freshness\_sla\_seconds** : Provides the expected SLO in the alert payload, so the responder doesn't need to look it up. \* **last\_pass\_evidence\_id** : Gives a direct pointer to the last good record, helping to pinpoint the time of failure. \#\#\#\#\# 4.3. Contract Compliance Signal \* **Purpose:** To detect when the rate of records failing data contract validation for a given dataset exceeds a defined Service Level Objective (SLO), indicating a potential breaking change or bug in a producer service. \* **Inputs:** Evidence events, specifically the validation.result field (PASS or FAIL) and the failed\_gates field (to confirm the failure was due to CONTRACT). \* **Computation Logic:** Over a sliding time window (e.g., 5 minutes), the engine computes the Contract compliance rate \= PASS\_count / (PASS\_count \+ FAIL\_count). It only includes records where the failure was explicitly a contract violation. \* **Incident Threshold Policy:** An incident is created when the compliance rate for a dataset drops below its SLO for a sustained period. For example: a compliance rate of less than 99.5% for 10 minutes for a Tier-1 dataset results in a SEV-1 incident. \* **Incident Payload Schema:** \* **compliance\_rate** **vs.** **slo\_threshold** : Quantifies the severity of the breach, showing how far below the 99.5% SLO the dataset has fallen. \* **evidence\_sample** : Provides direct links to failing Evidence records, allowing an engineer to immediately see the reason codes (e.g., MISSING\_FIELD:customer\_id) and debug the issue. \#\#\#\#\# 4.4. PII Signal \* **Purpose:** To detect the presence of disallowed Personally Identifiable Information (PII), Payment Card Industry (PCI), or Protected Health Information (PHI) data and trigger an immediate, high-severity security response. \* **Inputs:** Evidence events where validation.result is FAIL and failed\_gates contains PII. The engine also consumes the pii.classification and pii.fields data from the evidence. \* **Computation Logic:** The engine aggregates PII failure evidence, counting occurrences by PII classification type (PCI, PHI, PII) and collecting the names of the offending fields. Unlike other signals, PII violations often trigger incidents based on a single occurrence rather than a rate. \* **Incident Threshold Policy:** The incident response is driven by the classification of the data discovered. | Classification | Threshold | Severity | Required Action | | \------ | \------ | \------ | \------ | | PCI | \>= 1 event | SEV-1 | Page Security On-call \+ Auto-Quarantine | | PHI | \>= 1 event | SEV-1 | Page Security On-call \+ Auto-Quarantine | | PII | \>= 10 events / 5 min | SEV-2 | Create ticket \+ Auto-Quarantine | \* **Incident Payload Schema:** \* **classification** : Immediately informs the responder of the data's sensitivity (PCI is more critical than PII). \* **fields** : Lists the exact field names containing the sensitive data, accelerating investigation and remediation. \* **quarantine\_prefix** : Confirms that containment actions were taken and provides the S3 path for security teams to perform forensic analysis. These detailed designs provide the blueprint for implementation, which must be supported by robust handling of cross-cutting operational concerns. \#\#\#\# 5.0 Cross-Cutting Concerns \#\#\#\#\# 5.1. Observability and Telemetry As core components of an observability platform, the Signal Engines themselves must be highly observable to ensure their correctness and performance. A minimum set of telemetry is required for any engine implementation. \#\#\#\#\#\# Metrics All engines must export the following metrics to New Relic, labeled with dataset\_urn and signal\_type: \* signal\_eval\_latency\_ms: A histogram measuring the time taken to process a batch of evidence and compute a signal. \* evidence\_consumer\_lag: A gauge reporting the Kafka consumer lag for the engine's consumer group, indicating how far behind it is from the head of the Evidence Bus. \* incidents\_created\_total: A counter, labeled by severity, for every incident opened by the engine. \* db\_write\_latency\_ms: A histogram measuring latency for writes to DynamoDB and Neptune. \#\#\#\#\#\# Tracing Every engine's processing of an Evidence event must create a new child span. This span MUST be linked to the original trace via the trace\_id provided in the otel block of the EvidenceEvent. This creates an unbroken, end-to-end trace from the producer service through the enforcement layer and into the signal computation, which is critical for debugging the observability platform itself. \#\#\#\#\#\# Logging All log output MUST be structured (JSON). Every log statement related to the processing of a specific event or dataset must include the dataset\_urn, signal\_type, and evidence\_id to allow for fast correlation and debugging in a log analytics platform. \#\#\#\#\# 5.2. Failure Modes and Resilience The engines are designed to be resilient to upstream failures and transparent about their own operational state. The following failure modes must be handled gracefully. | Failure Mode | Detection | Required Behavior | | \------ | \------ | \------ | | **Evidence Bus Unavailable** | Kafka client connection errors. | Retry connections with exponential backoff. Increment evidence\_bus\_unavailable metric and trigger a high-severity platform alert if sustained. | | **High Consumer Lag** | Kafka lag metrics (evidence\_consumer\_lag) exceed a threshold. | HPA scales out pods. If lag persists, create a high-severity OBSERVABILITY\_PIPELINE\_LAG incident and suppress potentially false freshness alerts. | | **"Evidence Gap" (Policy Enforcer Down)** | No Evidence arrives for an active dataset for a period exceeding its SLA. | Create a high-severity OBSERVABILITY\_PIPELINE\_DOWN incident, attributing the issue to the enforcement layer to distinguish it from a standard freshness problem. | | **Schema Registry Unavailable** | Policy Enforcer emits Evidence with reason\_code: SCHEMA\_REGISTRY\_UNAVAILABLE. | The engine must handle this specific evidence type. It should not treat this as a producer contract failure but rather an observability platform dependency failure. Trigger a platform health alert. | | **DynamoDB Throttling / Write Failures** | SDK errors and high latency on db\_write\_latency\_ms metric. | Retry writes with exponential backoff. Prioritize signal computation over state persistence. Emit db\_write\_failures\_total metric and alert if sustained. | | **Neptune Unavailable** | Graph client connection or write errors. | Neptune writes must be asynchronous and non-blocking. Log the error, increment a neptune\_write\_failures\_total metric, and continue processing. RCA capability will be degraded, which should trigger a platform alert. | \#\#\#\#\# 5.3. Testing Strategy A comprehensive testing strategy is mandatory to ensure the correctness and reliability of the Signal Engines. \* **Unit Tests:** All core signal computation logic, including windowing, aggregation, and thresholding functions, must be covered by unit tests with clear inputs and expected outputs. \* **Contract Tests:** Each engine must have a suite of tests that consume mock Evidence events from a schema registry. This suite must include both "golden path" PASS cases and a variety of FAIL cases to verify that the correct SignalEvent or incident is (or is not) produced as expected. This validates the engine's adherence to its input contract. \* **Integration Tests:** Automated tests must be in place to verify the engine's ability to connect to and correctly write data to its dependencies, primarily DynamoDB and Neptune, using local or ephemeral instances in the CI/CD pipeline. \#\#\#\# 6.0 End-to-End Example: Contract Violation \#\#\#\#\# 6.1. Scenario Overview A developer on the Orders Service team deploys a hotfix that introduces a bug: the customer\_id field is accidentally omitted from the orders.created event payload. This is a direct violation of the data contract for the urn:dp:orders:created dataset. This section traces how the Signal Factory deterministically detects, reports, and provides context for this failure without any manual intervention. \#\#\#\#\# 6.2. Step-by-Step Flow 1\. **Evidence Emission:** The Policy Enforcer consumes the malformed raw event from the Orders Service. Its contract validation gate fails. The enforcer does not block the record (per Pattern 1), but it immediately emits a FAIL Evidence event to the Evidence Bus. \* **Example** **EvidenceEvent** **Payload:** 2\. **Signal Computation:** The **Contract Compliance Signal Engine** consumes this Evidence event. As more malformed events are produced, the engine's internal metric for the urn:dp:orders:created dataset, compliance\_rate, rapidly drops. Within minutes, it falls below the configured SLO of 99.5%. 3\. **Incident Creation:** Upon detecting the sustained SLO breach, the engine declares a SEV-1 incident. It generates an incident payload containing all the necessary context for an SRE to immediately understand the issue's scope and impact. \* **Example** **Incident** **Payload:** 4\. **Data Store Updates:** Simultaneously, the engine writes state and relationship data to the backend stores: \* **DynamoDB:** It updates the SignalState table for the dataset to severity: SEV-1 and status: OPEN. It also creates a new entry in the IncidentIndex table. \* **Neptune:** It writes the causal relationships to the graph: (Incident)-:TRIGGERED\_BY-\>(Signal) and (Signal)-:DERIVED\_FROM-\>(Evidence). This connects the high-level incident directly back to the specific, immutable evidence of the contract failures, enabling the RCA Copilot to provide a deterministic explanation.  
