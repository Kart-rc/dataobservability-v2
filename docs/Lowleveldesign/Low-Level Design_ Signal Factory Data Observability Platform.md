### Low-Level Design: Signal Factory Data Observability Platform \#\#\#\# 1.0 Introduction and Architectural Intent This document provides the complete low-level design for the Signal Factory's Data Observability Platform. The design is architected around a non-negotiable constraint: the existing central streaming platform and its producers cannot be modified. To provide observability and governance without altering this core infrastructure, this LLD adopts **Pattern 1: Out-of-Band Enforcement** . This pattern introduces two new, decoupled components that operate externally to the existing data flow: 1\. **The Gateway Control Plane:** An EKS service that provides APIs for managing all policies, contracts, schema bindings, and dataset configurations. It is the authoritative "brain" of the system. 2\. **The Policy Enforcer:** A scalable EKS stream processing service that consumes events from the raw topics of the central platform. It applies a deterministic pipeline of validation gates to every message *after* it has been published, establishing a definitive, record-level source of truth. This architecture represents a strategic shift from *pre-publish prevention* to *pre-consumption safety* . Instead of blocking data at a new front door, we add a powerful observability layer that creates an immutable evidence record for every event. This enables downstream consumers to make trust-based decisions, provides SREs with deterministic root-cause analysis, and allows the business to govern data quality without disrupting existing systems. This document details the complete, build-ready design for these components, their interfaces, data models, and operational requirements. \-------------------------------------------------------------------------------- \#\#\#\# 2.0 Component Responsibilities and Boundaries A clear delineation of responsibilities is critical for this decoupled architecture. The Enforcement Layer establishes immutable, per-record truth, while the Signal Processing Engines transform that truth into system-level health signals and incidents. This ensures that prevention logic and alerting logic never compete or drift. \#\#\#\#\# 2.1 Logical Component Roles | Component | Role | | \------ | \------ | | **Policy Enforcer** | **Establishes "Record Truth."** Observes raw events, evaluates deterministic per-record gates (schema, contract, PII), and emits immutable evidence for every record. | | **Signal Engines** | **Establish "Health Truth."** Consume evidence, compute windowed and anomaly-based signals (volume, freshness), enforce SLO thresholds, and create incidents. | | **Gateway Control Plane** | **Provides Authoritative Configuration.** Manages dataset policies, schema/contract bindings, and signal SLOs via a centralized API. | | **RCA Copilot** | **Provides Causal Explanation.** Uses evidence, signals, and traces to deterministically explain the root cause of an incident. | \#\#\#\#\# 2.2 Decision Matrix: Where Logic Belongs | Logic | Policy Enforcer | Signal Engine | | \------ | \------ | \------ | | JSON parsing / envelope normalization | ✅ | ❌ | | Schema validation | ✅ | ❌ | | Required fields, constraints per record | ✅ | ❌ | | PII detection per record | ✅ | ❌ | | Emit PASS/FAIL evidence for each record | ✅ | ❌ | | Compute compliance rate over time | ❌ | ✅ | | Static threshold breach → incident | ❌ | ✅ | | Dynamic anomaly detection | ❌ | ✅ | | Correlate multiple signals | ❌ | ✅ | | Provide root cause narrative | ❌ | ✅ | The following sections provide the detailed design for the components in scope for this LLD. \-------------------------------------------------------------------------------- \#\#\#\# 3.0 Scope and Boundaries Defining the scope ensures a focused, implementable design. This LLD covers the foundational Enforcement Layer and its Control Plane. | In Scope | Non-Goals | | \------ | \------ | | **Full LLD for the Policy Enforcer:** A complete, build-ready design for the stream processing service, including its gate pipeline and backend integrations. | The full low-level design of downstream **Signal Engines** (Freshness, Volume, Drift). | | **Full LLD for the Gateway Control Plane:** Detailed API specifications for managing dataset policies, resolution maps, and enforcement rules. | The full low-level design of the **RCA Copilot** and its analytical models. | | **Canonical Data Models:** The definitive schemas for the Evidence Event, Dataset Policy, and Dataset Resolution Map. | The full design of the **Neptune knowledge graph** and other persistence layers. | | **Producer Identity and Dataset Resolution:** Strategies for attributing events to producers and datasets in an out-of-band model. | The design of producer-side SDKs or interceptors. | | **Deployment on EKS:** A complete architecture for deploying the Enforcer and Control Plane on EKS, including scaling models and release strategies. | | | **"Steel-Thread" Reference Flow:** A concrete, end-to-end example demonstrating how an event flows from the raw stream through the Enforcer to evidence. | | | **Observability and Resiliency:** Definition of SLOs, SRE-grade metrics, tracing, alerts, and failure modes for the new components. | | This scope enables a focused implementation of the core observability layer, which will emit all necessary primitives to support the downstream components that are currently out of scope. \-------------------------------------------------------------------------------- \#\#\#\# 4.0 Core Architecture: The Post-Publish Gate Pipeline The core function of the **Policy Enforcer** is a deterministic, ordered pipeline of validation gates. Unlike an inline gateway, this pipeline executes *after* an event has been published to the central platform. Its purpose is not to block publication but to create an immediate, authoritative judgment on the event's quality and compliance. This "pre-consumption safety" model is fundamental to the platform's strategy. \`\`\`mermaid flowchart LR CSP\[Central Streaming Platform

Raw Topics\] \--\> PEPolicy Enforcer(EKS) subgraph PE direction LR G1Dataset Resolution \--\> G2Producer Identity \--\> G3Schema Gate \--\> G4Contract Gate \--\> G5PII Gate \--\> G6Emit Evidence end G6 \--\> EBEvidence Bus(Kafka/MSK) G6 \--\> S3QS3 Quarantine(Optional)  The sequential stages of the pipeline are as follows: 1\. \*\*Dataset Resolution Gate:\*\* Maps the raw source topic and event metadata to a canonical dataset\_urn. 2\. \*\*Producer Identity Gate:\*\* Attributes the event to a producer identity with a corresponding confidence score. 3\. \*\*Envelope/Parsing Gate:\*\* Parses the message payload and validates its basic structure. 4\. \*\*Schema Gate:\*\* Validates the message payload against the registered writer schema for syntactic correctness. 5\. \*\*Compatibility Gate:\*\* Checks that the writer's schema is compatible with the registered schema evolution policy. 6\. \*\*Contract Gate:\*\* Enforces business-level rules, such as the presence of required fields and domain constraints. 7\. \*\*PII Gate:\*\* Detects and validates the presence of sensitive data against the dataset's PII policy. 8\. \*\*Emit Evidence:\*\* An immutable evidence event is generated and published, capturing the outcome (PASS or FAIL) and the results of the gate pipeline execution. \#\#\#\#\# Gate Enforcement Modes Because the system is out-of-band, "enforcement" refers to the actions taken \*after\* a gate failure is detected. This behavior is governed by a policy-driven enforcement mode defined per dataset: \* \*\*ALERT\_ONLY\*\* \*\*:\*\* The default mode. On gate failure, FAIL evidence is emitted, and alerts are generated by downstream Signal Engines. \* \*\*QUARANTINE\_ON\_FAIL\*\* \*\*:\*\* In addition to emitting FAIL evidence, the failed raw message and its validation metadata are written to a secure S3 bucket for forensic analysis or replay. \* \*\*REPUBLISH\_PASS\_ONLY\*\* \*\*:\*\* An optional mode where only events that pass all gates are republished to a separate, "blessed" topic or data store for consumption by sensitive downstream applications. This pipeline architecture is configured and managed via the formal API contracts exposed by the Gateway Control Plane. \-------------------------------------------------------------------------------- \#\#\#\# 5.0 API Interfaces and Contracts (Gateway Control Plane) The REST API of the \*\*Gateway Control Plane\*\* serves as the formal, versioned contract for managing all observability policies. It is an administrative API used by platform teams and CI/CD systems, not a real-time data ingestion endpoint. Its design prioritizes declarative configuration, policy versioning, and operational safety. \#\#\#\#\# PUT /v1/datasets/{dataset\_urn}/policy This is the primary endpoint for creating or updating the complete policy for a given dataset. It is idempotent and version-aware. | Element | Type | Description | | \------ | \------ | \------ | | \*\*Path Parameter\*\* | | | | dataset\_urn | String | \*\*Required.\*\* The unique, namespaced identifier for the dataset (e.g., urn:dp:orders:created). | | \*\*Request Header\*\* | | | | Authorization | String | \*\*Required.\*\* A Bearer for an authorized administrative principal. | | \*\*Request Body\*\* | JSON Object | \*\*Required.\*\* The complete Dataset Policy Model object (see Section 6.2). | \#\#\#\#\# Response Codes and Semantics | HTTP Code | Status | Reason for Use | | \------ | \------ | \------ | | 200 OK | \*\*Success\*\* | The policy was successfully updated. | | 201 Created | \*\*Success\*\* | A new policy for the dataset was successfully created. | | 400 Bad Request | Client Error | The request body is malformed or fails validation against the policy schema. | | 401 Unauthorized | Client Error | Authentication failed. | | 403 Forbidden | Client Error | The authenticated principal is not authorized to manage policies for this dataset. | | 422 Unprocessable Entity | Client Error | The policy is syntactically valid but contains logical errors (e.g., references a non-existent schema). | | 500 Internal Server Error | Server Error | The control plane failed to persist the policy update. | \#\#\#\#\# Other Key Endpoints \* \*\*GET /v1/datasets/{dataset\_urn}/policy\*\* \*\*:\*\* Retrieves the active policy for a dataset. \* \*\*PUT /v1/datasets/{dataset\_urn}/resolution\*\* \*\*:\*\* Manages the rules for mapping raw topics/events to this dataset\_urn. \* \*\*GET /v1/evidence?dataset\_urn=...\*\* \*\*:\*\* An optional query endpoint (backed by an evidence index) for debugging and status checks. \#\#\#\#\# Kubernetes Probes \* \*\*GET /healthz\*\* \*\*:\*\* Liveness probe for the Control Plane service. \* \*\*GET /readyz\*\* \*\*:\*\* Readiness probe, checking connectivity to the underlying policy store (e.g., DynamoDB). These APIs manage the canonical data models that drive the Policy Enforcer's behavior. \-------------------------------------------------------------------------------- \#\#\#\# 6.0 Canonical Data Models and Schemas Standardized data models are strategically vital. They form the machine-readable basis for all policy enforcement by the Policy Enforcer and ensure interoperability for all downstream observability components. \#\#\#\#\# 6.1 Evidence Event (The Primary Contract) This event is the primary output of the Policy Enforcer, creating an immutable audit record for every observed publish attempt. It is the foundational data source for all downstream Signal Engines. \*\*Target Topic:\*\* signal\_factory.evidence json { "evidence\_id": "evd-01JH...", "timestamp": "2026-01-13T12:01:02.120Z", "stage": "OBSERVED\_RAW", "dataset\_urn": "urn:dp:orders:created", "producer": { "producer\_id": "orders-svc", "identity\_confidence": "HIGH" }, "source": { "raw\_topic": "raw.orders.events", "partition": 12, "offset": 99817263, "key": "o-784233", "event\_time": "2026-01-13T12:01:01.900Z" }, "schema": { "schema\_id": "glue:orders.created:17" }, "contract": { "contract\_id": "dc:orders.created:3" }, "validation": { "result": "PASS", "failed\_gates": , "reason\_codes":  }, "otel": { "trace\_id": "2f1c3f...", "span\_id": "c301..." }, "fingerprints": { "payload\_hash": "sha256:...", "schema\_fingerprint": "..." } }  \#\#\#\#\# 6.2 Dataset Policy Model This model is the source of truth for all validation, routing, and governance rules applied by the Policy Enforcer. It is managed via the Control Plane API and stored in DynamoDB. | Field | Type | Example | Notes | | \------ | \------ | \------ | \------ | | dataset\_urn | string | urn:dp:orders:created | Primary key for the policy. | | schema\_id | string | glue:orders.created:17 | The default writer schema ID in the registry. | | contract\_id | string | dc:orders.created:3 | The versioned data contract ID. | | enforcement\_mode | enum | QUARANTINE\_ON\_FAIL | Action to take on failure (ALERT\_ONLY, QUARANTINE\_ON\_FAIL). | | pii\_policy | enum | BLOCK | Policy for handling PII. BLOCK implies quarantine/alerting. | | owners | list | \[orders-team\] | Owning team for notifications and governance. | | tier | string | TIER\_1 | Operational tier, used to drive SLOs and alert severity. | | freshness\_slo\_sec | int | 120 | Expected max latency between events for this dataset. | | volume\_rpm\_expected | int | 1000 | Expected events per minute at peak, for volume signal baselining. | \#\#\#\#\# 6.3 Dataset Resolution Map This model is critical for the out-of-band architecture, as it provides the rules for mapping raw, un-annotated events from the central platform to a canonical dataset. | Field | Type | Example | Notes | | \------ | \------ | \------ | \------ | | source\_type | enum | TOPIC | The type of source identifier (TOPIC, SCHEMA\_ID, etc.). | | source\_value | string | raw.orders.events | The value of the source identifier. | | dataset\_urn | string | urn:dp:orders:created | The target dataset this source maps to. | | resolution\_confidence | enum | HIGH | The confidence level of this mapping rule. | | extraction\_rule | string | event\_type=orders.created | (Optional) Further rule to disambiguate within a topic. | These data models drive the Policy Enforcer's integration with its critical backend systems. \-------------------------------------------------------------------------------- \#\#\#\# 7.0 Backend Integration Details The Policy Enforcer operates as a classic stream processor, acting as both a consumer from the central platform and a producer to the observability event bus. \#\#\#\#\# 7.1 Kafka/MSK Consumer Configuration (Enforcer ← Raw Topics) The Enforcer's reliability depends on its ability to consume the raw data stream without loss. Key settings include: \* \*\*group.id\*\* \*\*:\*\* A unique consumer group ID per domain (e.g., signal-factory-enforcer-orders) to enable parallel processing and isolate failure domains. \* \*\*auto.offset.reset=latest\*\* \*\*:\*\* To avoid reprocessing huge backlogs on startup; recovery and backfill are handled as separate operational procedures. \* \*\*enable.auto.commit=false\*\* \*\*:\*\* To ensure offsets are committed only after evidence has been successfully emitted, providing at-least-once processing guarantees. \#\#\#\#\# 7.2 Kafka/MSK Producer Configuration (Enforcer → Evidence Topic) To ensure the durability of evidence events, the Enforcer's internal Kafka producer is configured for high reliability: \* \*\*acks=all\*\* \*\*:\*\* \*\*Why:\*\* To guarantee the highest level of durability, ensuring the write is acknowledged only after all in-sync replicas have received the message. \* \*\*enable.idempotence=true\*\* \*\*:\*\* \*\*Why:\*\* To ensure exactly-once write semantics from the Enforcer to the evidence topic, preventing duplicate evidence during retries. \* \*\*retries=Integer.MAX\_VALUE\*\* \*\*:\*\* \*\*Why:\*\* To retry indefinitely until the delivery timeout is reached, coupled with a bounded delivery.timeout.ms to prevent infinite blocking. \#\#\#\#\# 7.3 Other Integrations \* \*\*Schema Registry (Glue):\*\* The Enforcer is a client to the schema registry, fetching schemas to perform validation. It employs an in-memory cache with a short TTL (1-5 minutes) to reduce latency and improve resilience against registry unavailability. \* \*\*Policy Store (DynamoDB):\*\* The Enforcer caches dataset policies fetched from the Control Plane's store to ensure low-latency gate execution. These integrations are hosted within a Kubernetes-based deployment architecture. \-------------------------------------------------------------------------------- \#\#\#\# 8.0 Deployment Architecture and Scaling (EKS) The Policy Enforcer and Gateway Control Plane are designed as cloud-native services deployed on Amazon EKS, optimized for high availability, automated scaling, and operational resilience. \#\#\#\#\# 8.1 Policy Enforcer Deployment \* \*\*Deployment\*\* \*\*:\*\* Manages the Enforcer's pods, enabling declarative updates and rollbacks. \* \*\*HorizontalPodAutoscaler (HPA)\*\* \*\*:\*\* Automatically scales the number of pods up or down. The primary scaling metric is \*\*Kafka consumer lag\*\* , ensuring the Enforcer can keep up with ingress volume. CPU utilization is used as a secondary metric. \* \*\*PodDisruptionBudget (PDB)\*\* \*\*:\*\* Ensures a minimum number of pods (e.g., minAvailable: 1\) remain available during voluntary disruptions, guaranteeing high availability. \* \*\*ConfigMap\*\* \*\*/\*\* \*\*Secret\*\* \*\*:\*\* Manages configuration, such as topics to consume, and securely stores credentials for MSK and the schema registry. \* \*\*NetworkPolicy\*\* \*\*:\*\* Restricts egress traffic to only required endpoints (MSK, registries, New Relic). \#\#\#\#\# 8.2 Gateway Control Plane Deployment The Control Plane is a standard stateless API service with a smaller, more stable resource footprint. \* \*\*Deployment\*\* \*\*:\*\* Manages a small, fixed number of replicas (e.g., 2-3) for high availability. \* \*\*Service\*\* \*\*/\*\* \*\*Ingress\*\* \*\*:\*\* Exposes the administrative API internally. \* \*\*HPA\*\* \*\*:\*\* Can be used based on CPU or request rate, though traffic is expected to be low and predictable. \#\#\#\#\# 8.3 Release Strategy A canary release process is used for the Policy Enforcer to deploy new versions safely. \* \*\*Process:\*\* A new version is deployed as a separate consumer group, initially processing a small, non-critical topic. It is gradually scaled up to consume a percentage of production traffic. \* \*\*Automated Rollback:\*\* The release is automatically rolled back if key health indicators are breached, such as a high rate of evidence emission failures, a significant increase in evaluation latency, or a sustained increase in consumer lag. Effective operation of this architecture requires a comprehensive observability strategy. \-------------------------------------------------------------------------------- \#\#\#\# 9.0 Observability, SLOs, and Alerting The Policy Enforcer is designed to be "SRE-grade" from inception, with comprehensive observability built in. This is crucial for meeting SLOs, debugging issues, and understanding system behavior. \#\#\#\#\# 9.1 OpenTelemetry Tracing Distributed tracing provides a detailed view of an event's lifecycle within the Enforcer. \* \*\*Trace Propagation:\*\* If an incoming raw message contains a traceparent header, the Enforcer joins that trace. If not, it starts a new one. \* \*\*Required Spans:\*\* enforcer.consume, enforcer.resolve\_dataset, enforcer.validate.schema, enforcer.validate.contract, enforcer.emit.evidence. \* \*\*Critical Attributes:\*\* Every span must be annotated with dataset\_urn, raw\_topic, result, failed\_gate, and identity\_confidence. \#\#\#\#\# 9.2 SRE-Grade Metrics | Metric | Type | Labels | Why | | \------ | \------ | \------ | \------ | | enforcer\_consumer\_lag | Gauge | topic, partition | \*\*Primary scaling and health metric.\*\* Measures processing delay. | | enforcer\_records\_total | Counter | topic, dataset, result | Tracks overall volume and PASS/FAIL rates. | | enforcer\_gate\_failures\_total | Counter | gate, reason, dataset | Measures prevention effectiveness and identifies specific contract issues. | | enforcer\_eval\_latency\_ms | Histogram | dataset | To measure and alert on the processing latency SLO. | | enforcer\_evidence\_emit\_errors\_total | Counter | topic, error | Monitors health of the connection to the evidence bus. | | policy\_cache\_hit\_ratio | Gauge | | Monitors the performance of the in-memory dataset policy cache. | \#\#\#\#\# 9.3 Structured Logging All logs are emitted as structured JSON. Every log entry must include trace\_id, dataset\_urn, and the raw message's source coordinates (topic, partition, offset). Payloads are not logged by default. \#\#\#\#\# 9.4 Service Level Objectives (SLOs) and Alerts \* \*\*Evidence Completeness:\*\* ≥ 99.99% of raw messages observed produce an evidence event. \* \*\*Evidence Latency:\*\* p95 \< 2 seconds from raw topic ingest to evidence emission. \*\*Example New Relic Alert Conditions:\*\* \* \*\*Critical Lag:\*\* Consumer lag \> 1000 for 5 minutes on a Tier-1 topic. \* \*\*Evidence Gap:\*\* Evidence emission rate for a dataset is 0 for 5 minutes during an expected traffic window. \* \*\*Contract Breakage Spike:\*\* Anomaly detected on enforcer\_gate\_failures\_total where gate=CONTRACT. \* \*\*Registry Unavailability:\*\* Spike in gate failures with reason=SCHEMA\_REGISTRY\_UNAVAILABLE. \-------------------------------------------------------------------------------- \#\#\#\# 10.0 Security and Governance In an out-of-band model, security shifts from authenticating API calls to attributing identity from the raw stream and governing data based on detected content. \*\*Producer Identity Resolution\*\* This is the most critical security function. The Enforcer establishes producer identity with a confidence score using a prioritized strategy: 1\. \*\*High Confidence:\*\* From trusted transport-level principals (e.g., Kafka principal via mTLS or IAM). 2\. \*\*Medium Confidence:\*\* From a curated map of topic ownership (e.g., raw.orders.events is owned by orders-svc). 3\. \*\*Low Confidence:\*\* From a producer-ID field within the payload or headers, which may be spoofed. The identity\_confidence field in every evidence event is crucial for downstream security forensics. \*\*Data Classification and Governance\*\* The Enforcer is the central point for applying data classification policies. Based on the pii\_policy in the dataset definition, it can detect unauthorized sensitive data. When the enforcement\_mode is QUARANTINE\_ON\_FAIL, it actively removes this data from the main analytical path by shunting it to a secure, restricted-access location, serving as a critical compensating control. \*\*Audit\*\* The immutable stream of evidence events published to signal\_factory.evidence creates a comprehensive, non-repudiable audit trail. This log details every validation decision made for every observed event, which is essential for compliance, security forensics, and proving governance enforcement. \-------------------------------------------------------------------------------- \#\#\#\# 11.0 Failure Modes and Handling Proactively designing for failure is key to meeting the system's high availability SLOs. | Failure mode | Detection | User impact | Mitigation | Runbook action | | \------ | \------ | \------ | \------ | \------ | | \*\*Policy Enforcer is down\*\* | enforcer\_consumer\_lag grows; evidence gap alert | No evidence is generated; downstream signals are stale. | K8s deployment ensures restart; PDB prevents disruption; HPA scales to meet load. | Check pod status, logs, and resource utilization. Scale replicas if necessary. | | \*\*Registry unavailable\*\* | Spike in gate.schema failures with specific reason code | Schema validation fails. | Cache last-known good schemas; mark evidence SCHEMA\_REGISTRY\_UNAVAILABLE. | Restore registry connectivity; temporarily increase cache TTL if needed. | | \*\*Policy Store unavailable\*\* | Readiness probe fails; metric for policy lookup errors | The Enforcer cannot fetch policy updates. | Cache last-known good policies for a configured TTL; operate in a safe default mode. | Restore connectivity to DynamoDB. | | \*\*Consumer lag grows\*\* | enforcer\_consumer\_lag metric breaches threshold | Evidence and downstream signals are delayed. | HPA automatically scales out pods. | Investigate cause of slowness (bad message, dependency latency) or adjust HPA thresholds. | | \*\*Upstream producer regression\*\* | Spike in gate.contract or gate.schema failures for a dataset | A high volume of data is invalid. | Quarantine mode automatically sequesters bad data; alerts fire for Signal Engines. | Notify the producing team and provide links to failed evidence samples. | | \*\*Evidence Bus unavailable\*\* | Spike in enforcer\_evidence\_emit\_errors\_total | Evidence is lost or delayed. | Bounded in-memory buffer with retries. Shed load if backpressure is sustained. | Check MSK cluster health, ISR count, and networking. | \-------------------------------------------------------------------------------- \#\#\#\# 12.0 End-to-End Reference Flow: Orders Service Example This section walks through a concrete example of the "Orders Service" publishing an event, demonstrating the out-of-band pipeline in action. mermaid sequenceDiagram autonumber participant OS as Orders Service participant CSP as Central Streaming Platform (unchanged) participant PE as Policy Enforcer (EKS) participant EB as Evidence Bus (MSK) participant SE as Signal Engines OS-\>\>CSP: Publish business event to raw.orders.events PE-\>\>CSP: Consume raw.orders.events PE-\>\>PE: Run Gate Pipeline (Resolve, Identify, Validate) alt Successful Validation PE-\>\>EB: Emit Evidence (PASS) else Failed Validation PE-\>\>EB: Emit Evidence (FAIL) PE--\>\>PE: Quarantine Payload (Optional) end SE-\>\>EB: Consume Evidence SE-\>\>SE: Aggregate & Compute Signals \`\`\` \#\#\#\#\# 12.1 Configuration \* **Dataset Resolution Map:** Maps topic raw.orders.events to urn:dp:orders:created. \* **Dataset Policy:** contract\_id is dc:orders.created:3, enforcement\_mode is QUARANTINE\_ON\_FAIL. \* **Contract:** Requires fields order\_id and customer\_id. \#\#\#\#\# 12.2 Successful Event Publication 1\. **Publish to Central Platform:** The Order Service publishes a valid event containing both order\_id and customer\_id to the raw.orders.events topic. The platform accepts it. 2\. **Policy Enforcer Consumption:** The Enforcer consumes the raw message. 3\. **Gate Pipeline Execution:** \* Dataset Resolution: **PASS** (maps to urn:dp:orders:created). \* Producer Identity: **PASS** (identifies orders-svc with HIGH confidence). \* Schema Gate: **PASS** . \* Contract Gate: **PASS** (all required fields are present). 4\. **Evidence Emission:** The Enforcer generates and publishes a PASS evidence event to the signal\_factory.evidence topic. The event details the successful validation and includes the source coordinates (raw topic/partition/offset). 5\. **Signal Engine Processing:** Downstream Signal Engines consume the PASS evidence, updating volume and freshness counters. No incident is created. \#\#\#\#\# 12.3 Failed Event (Pre-Consumption Safety in Action) 1\. **Publish of Bad Data:** A buggy deployment of the Order Service publishes an event **missing the** **customer\_id** **field** to raw.orders.events. The central platform, having no validation, accepts it. 2\. **Policy Enforcer Consumption:** The Enforcer consumes the malformed message. 3\. **Gate Pipeline Execution:** \* The pipeline proceeds until the **Contract Gate** , which **FAILS** because a required field is missing. 4\. **Evidence Emission & Action:** \* The Enforcer publishes a FAIL evidence event with failed\_gates: "CONTRACT" and reason\_codes: "MISSING\_FIELD:customer\_id". \* Because the policy is QUARANTINE\_ON\_FAIL, the Enforcer writes the raw failed message to the S3 quarantine bucket with the associated evidence metadata. 5\. **Signal Engine Processing:** \* The Contract Compliance Signal Engine consumes the FAIL evidence. \* It detects a drop in the compliance rate below the configured SLO. \* An incident is created, alerting the SRE team to the upstream breaking change *before* downstream consumers are impacted by the bad data. This flow demonstrates how the out-of-band architecture provides powerful governance and safety without modifying the core data transport. \-------------------------------------------------------------------------------- \#\#\#\# 13.0 Implementation Blueprint The implementation can be decomposed into two primary, parallelizable work packages. \* **Gateway Control Plane (APIs \+ Policy Registry):** \* Implement the DynamoDB models for dataset policies and resolution maps. \* Build the core REST API framework for managing these policies, including AuthN/AuthZ for administrative access. \* Develop a safe, automated mechanism for the Policy Enforcer to fetch and cache policy updates. \* **Policy Enforcer (Stream Consumer \+ Gate Pipeline):** \* Establish the core Kafka consumer framework for ingesting from raw topics. \* Develop the pluggable, ordered pipeline for executing validation gates. \* Build client integrations for the schema registry and contract registry. \* Implement the idempotent Kafka producer for publishing canonical evidence events. \* Develop the Helm chart for EKS deployment, including HPA configuration based on consumer lag. \* Configure dashboards and alerts in New Relic based on the SRE-grade metrics. By executing this blueprint, we will deliver a robust, scalable, and operationally excellent observability platform. This system will serve as the foundational pillar for data quality, safety, and governance within the entire Signal Factory ecosystem.  
