**DATA OBSERVABILITY PLATFORM v2.0**

Detailed 12-Month Implementation Roadmap

With Rationale, Examples, and Scalability Targets

4-Team Organization | January 2026 \- December 2026

# **Executive Context**

This document provides detailed monthly milestones for the Data Observability Platform implementation. Each milestone is grounded in specific requirements from the technical specification and includes:

* WHY: Business and technical rationale tied to platform objectives  
* EXAMPLE: Concrete scenario demonstrating milestone value  
* SCALE: Throughput, latency, and capacity targets

## **Requirements Traceability**

| Req ID | Requirement | Priority | Business Impact |
| ----- | ----- | :---: | ----- |
| REQ-OT-001 | Trace context propagation | P0 | Enables RCA causality—without it, failures cannot be traced across services |
| REQ-OT-003 | Producer identity header | P0 | Enables attribution—'who sent bad data?' is unanswerable without identity |
| REQ-DC-001 | Contract definition files | P0 | Enables Contract Engine—defines 'what correct data looks like' |
| REQ-FR-001 | Event/processing timestamps | P0 | Enables Freshness Engine—detects stale data and pipeline stalls |
| REQ-OT-002 | Span creation for key ops | P1 | Enables precise RCA—identifies which operation failed within a service |
| REQ-DQ-001 | Field-level quality checks | P1 | Enables DQ Engine—catches semantic issues beyond structural validity |
| REQ-VOL-001 | Throughput metrics emission | P1 | Enables Volume Engine—detects drops/spikes as leading failure indicators |

## **Platform Scalability Targets**

| Dimension | Day 1 Target | Scale Target (M12) | Autoscale Trigger |
| ----- | ----- | ----- | ----- |
| Event Throughput | 10K events/sec | 50-500K events/sec | Kafka lag \>30s OR CPU \>70% |
| Evidence Latency | \<2s p99 | \<2s p99 (maintained) | N/A (latency SLO) |
| Signal Latency | 5-min windows | 5-min windows (maintained) | Lag \>60s |
| RCA Query Latency | \<2 min | \<2 min (maintained) | N/A (query SLO) |
| Evidence Bus Partitions | 64 partitions | 64-256 partitions | Partition fill \>80% |
| Datasets Supported | 50 (Tier-1) | 10,000 total | Registration required |

# **Team 1: Platform Core — Detailed Milestones**

Team 1 owns the Enforcement Plane: Policy Enforcer (5-gate pipeline), Gateway Control Plane, and Evidence Bus. The team establishes per-record truth that feeds all downstream systems.

| Mo | Milestone | Deliverables | Why (Rationale) | Example | Scale |
| :---: | ----- | ----- | ----- | ----- | ----- |
| **M1** | **Infrastructure Foundation** | EKS clusters, Kafka topics (signal\_factory.evidence with 64 partitions), DynamoDB tables (DatasetRegistry, DatasetResolutionMap), Policy Enforcer skeleton with G1 (Resolution) gate | G1 Resolution gate establishes the URN-based identity layer. Without deterministic topic→dataset\_urn mapping, all downstream correlation fails. 64 partitions enable 50K events/sec throughput from Day 1\. | When orders-svc publishes to raw.orders.events, G1 resolves it to urn:dp:orders:created. This URN becomes the join key across all systems. | 64 partitions × 1K events/sec/partition \= 64K events/sec capacity. Autoscaling triggers at Kafka lag \>30s or CPU \>70%. |
| **M2** | **Producer Attribution (G2 Identity)** | G2 Identity gate with header parsing (x-obs-producer-service), confidence scoring (HIGH/MEDIUM/LOW), Gateway Control Plane APIs, Evidence Bus schema v1.0 finalized | REQ-OT-003 (Producer identity header) is P0 because RCA cannot attribute failures without knowing WHO sent bad data. Confidence scoring handles legacy services lacking headers. | Evidence includes: producer\_id='orders-svc', confidence='HIGH' (header present) or confidence='MEDIUM' (inferred from topic pattern). | Header parsing adds \<1ms latency. Identity fallback map (topic→service) cached in DynamoDB with 100ms refresh. |
| **M3** | **Schema Validation (G3 Schema)** | G3 Schema gate with Glue Registry integration, deterministic schema fingerprinting (SHA-256 canonicalization), Evidence events include schema\_fingerprint field | Schema fingerprinting is the foundation of drift detection. REQ-DC-001 requires fingerprints to identify 'what changed' between last-good and first-bad. Deterministic canonicalization ensures reproducibility. | Fingerprint changes from sha256:9f2c... to sha256:a1b3... → drift detected. Evidence carries both fingerprints for RCA correlation. | Glue Registry cached locally with 5-min TTL. Fingerprint computation is O(n) on payload size, \<5ms for typical events. |
| **M4** | **Contract Validation (G4 Contract)** | G4 Contract gate with ODCS contract validation, baseline inference from 7-day production samples, standardized reason codes (MISSING\_FIELD, TYPE\_MISMATCH, NULL\_VIOLATION) | G4 is P0 for RCA because it catches structural breaks BEFORE downstream consumers fail. Reason codes enable automated remediation suggestions. | Contract specifies customer\_id: required. Event arrives with null → FAIL with reason\_code='MISSING\_FIELD:customer\_id'. Copilot can now suggest rollback. | Contract lookups from DynamoDB DatasetRegistry with \<10ms p99. Contracts cached per-topic in Enforcer memory. |
| **M5** | **PII Detection & Steel Thread** | G5 PII gate with pattern-based detection (SSN, email, phone), complete 5-gate pipeline operational, first Tier-1 service (orders-svc) onboarded end-to-end | Steel thread validates the entire path: Producer → Enforcer → Evidence → Engine → Signal → Incident. One real service proves integration before scaling. | orders-svc deploys v3.17 with missing field → Evidence FAIL → Contract Signal drops to 10% compliance → SEV-1 incident created → RCA shows deployment correlation. | Full 5-gate pipeline adds \<2s end-to-end latency. PII regex patterns compiled once at startup. |
| **M6** | **Tier-1 Critical Path** | 5 Tier-1 services producing evidence, Consumer trust layer with evidence lookup API, Enforcer HPA tuned for 10K events/sec baseline | Tier-1 services handle 80% of business value. Consumer trust API enables downstream systems to make evidence-based decisions (process vs. quarantine). | Finance pipeline queries: 'Is orders.created evidence PASS in last 5 min?' If \<95%, route to quarantine topic instead of processing. | 10K events/sec baseline with autoscale to 50K. HPA triggers: lag \>30s OR CPU \>70%. Scale-down at lag \<5s AND CPU \<30%. |
| **M7** | **Batch Evidence Support** | Spark job evidence emission via OpenLineage facets, DAG correlation linking (dag\_run\_id→evidence), Bronze-to-Silver pipeline instrumented with run\_id propagation | Batch pipelines process 60% of data volume but lack real-time observability. OpenLineage run\_id is the batch equivalent of OTel trace\_id for cross-job correlation. | Spark job orders-delta-landing emits OpenLineage with run\_id. If job fails, RCA traverses: dag\_run → spark\_job → dataset → upstream producer. | Batch evidence emitted at job completion, not per-record. Typical Spark job adds 10-20 evidence events regardless of data volume. |
| **M8** | **Stream Processing Evidence** | KStreams and Flink operator-level evidence emission, topology-to-evidence graph linkage, 50% of Tier-1 topics producing evidence | Streaming topologies have complex internal state. Operator-level evidence enables RCA to identify which processor stage failed. | Flink PaymentEnrichment operator emits evidence with topology\_id \+ operator\_name. Failure isolated to enrichment step, not entire job. | Stream evidence sampled at 1% for PASS, 100% for FAIL. Prevents evidence explosion from high-throughput streams. |
| **M9** | **Evidence Archive & Replay** | S3 archive with lifecycle policies (hot→warm→cold), Enforcer replay capability for incident forensics, 80% Tier-1 coverage achieved | Historical evidence enables retroactive RCA ('What was happening 3 days ago?'). Replay supports incident post-mortems and compliance audits. | P1 incident at 3am. Morning investigation replays evidence from 2-4am window to reconstruct failure timeline. | Hot tier: 24h in DynamoDB. Warm: 7d in S3 Standard. Cold: 90d in S3 Glacier. \~50TB/month estimated storage. |
| **M10** | **Multi-Region & Contract Versioning** | Multi-region evidence replication for DR (RPO \<1h), Enforcer latency optimized to \<2s p99, contract versioning and rollback support | DR ensures observability survives region failure. Contract versioning enables safe evolution without breaking consumers. | Contract v3 adds new required field. Services have 7-day grace period to adopt. Enforcer validates against both v2 and v3 during transition. | Cross-region replication via S3 CRR for archive, Kafka MirrorMaker for Evidence Bus. Failover RTO \<15min. |
| **M11** | **Self-Service Onboarding** | Full Tier-1 \+ Tier-2 coverage (100+ datasets), self-service onboarding portal for new topics, Enforcer observability dashboard (own-system metrics) | Self-service reduces Platform team bottleneck. Teams can onboard new topics without filing tickets. Dashboard shows Enforcer health to build trust. | Team registers new topic via portal → contract template auto-generated from samples → team reviews and approves → evidence flows within 1 hour. | Portal handles 10 new topic registrations/day. Contract inference runs as async batch job, not blocking. |
| **M12** | **Production Hardening** | Chaos testing (Enforcer failure, Registry unavailability), failure injection runbooks, Platform SLOs published (99.9% evidence availability, \<2s latency) | SLOs create accountability. Chaos testing validates resilience claims. Runbooks enable oncall to resolve issues without escalation. | Chaos test: Kill 50% of Enforcer pods. Expected: HPA scales up, lag spike \<2min, zero evidence loss. Actual matches expected → production ready. | SLO: 99.9% availability \= \<8.7h downtime/year. \<2s p99 latency at 50K events/sec sustained load. |

# **Team 2: Signal Processing — Detailed Milestones**

Team 2 owns the Processing and Knowledge Planes: Signal Engines (Freshness, Volume, Contract, DQ, Anomaly), Neptune graph, DynamoDB state stores, and Alerting. The team transforms per-record Evidence into aggregated Signals and actionable Incidents.

| Mo | Milestone | Deliverables | Why (Rationale) | Example | Scale |
| :---: | ----- | ----- | ----- | ----- | ----- |
| **M1** | **Signal Infrastructure Setup** | EKS deployments for Signal Engines, consumer groups (signal\_engines.freshness, .volume, .contract), DynamoDB tables (SignalState, IncidentIndex), Freshness Engine skeleton | Signal Engines consume Evidence Bus in parallel via separate consumer groups. This fan-out design enables independent scaling and deployment. | signal\_engines.freshness group has 6 instances, each consuming \~10 partitions. If freshness logic changes, only freshness pods redeploy. | Each engine type scales independently. Consumer group model ensures exactly-once processing semantics per partition. |
| **M2** | **Freshness Engine Production** | Freshness Engine with 5-minute tumbling windows, last\_event\_ts tracking per dataset, freshness SLO evaluation, first FRESHNESS signals emitted | REQ-FR-001 (timestamps) enables Freshness Engine. Pipeline stalls are the \#1 data incident type. Freshness signals detect stalls within 5 minutes vs. hours with manual monitoring. | Dataset orders.created SLO: max\_delay\_seconds=900. No events for 10min → FRESHNESS\_BREACH signal with severity=WARNING (approaching) or CRITICAL (exceeded). | 5-min tumbling windows balance latency vs. noise. Watermarking handles late-arriving events up to 2min grace period. |
| **M3** | **Volume Engine Production** | Volume Engine with event count aggregation, 7-day baseline computation, static threshold breach detection (min/max events per window) | REQ-VOL-001 (throughput metrics) feeds Volume Engine. A 50% volume drop is invisible without baselines. Volume anomalies are leading indicators of upstream failures. | Baseline: 10K events/hour. Current: 5K events/hour → VOLUME\_DROP signal. Investigation reveals: upstream producer deployment paused message publishing. | Baseline stored in DynamoDB per dataset. Rolling 7-day average computed incrementally, not full recomputation. |
| **M4** | **Contract Compliance Engine** | Contract Compliance Engine aggregating evidence failures, compliance rate computation per 5-min window, Schema drift signal with first\_bad/last\_good fingerprints | Contract Engine transforms per-record evidence FAILs into actionable compliance metrics. Schema drift signal is critical for 'what changed?' RCA. | 1,247 evidence events in window. 1,120 FAIL with reason=MISSING\_FIELD:customer\_id. Compliance rate=10.2% (SLO=95%) → CONTRACT\_BREACH incident. | Failure signatures deduplicated by (dataset\_urn \+ failed\_gates \+ reason\_codes). Prevents graph explosion from repetitive failures. |
| **M5** | **Neptune Graph Foundation** | Neptune cluster (r5.large HA), node types (Dataset, Service, Deployment, FailureSignature, Signal, Incident), causal edges (INTRODUCED, CAUSED, TRIGGERED) | Neptune enables the RCA traversal: Incident ← Signal ← FailureSignature ← Deployment. Without causal edges, Copilot cannot explain 'why'. | Gremlin query: g.V(incident).in('TRIGGERED').in('CAUSED').in('INTRODUCED') → returns Deployment v3.17 that introduced MISSING\_FIELD failure. | Cardinality controls: max 10K nodes/dataset, 1K edges/node. FailureSignature dedup prevents per-record graph writes. |
| **M6** | **Operational State & Correlation** | DynamoDB SignalState for fast dashboard queries, IncidentIndex for active incident lookup, Signal-to-Incident correlation automated, 5 Tier-1 datasets with full signals | DynamoDB serves operational queries (\<10ms). Neptune serves RCA queries. Dual storage separates 'what is happening now?' from 'why did it happen?' | Dashboard queries SignalState: 'Show all datasets with CRITICAL signals'. RCA queries Neptune: 'Trace causal path from incident to deployment'. | DynamoDB on-demand pricing. \~3K WCU for signal updates at 50K events/sec. Read capacity autoscales for dashboard load. |
| **M7** | **Data Quality Engine** | DQ Engine with null rate, uniqueness, validity checks per field, DQ signal types (NULL\_RATE\_BREACH, UNIQUENESS\_VIOLATION), PagerDuty integration prototype | REQ-DQ-001 (field-level checks) enables DQ Engine. Semantic data issues (valid but wrong) complement structural contract checks. | email field null\_rate baseline=0.1%. Current=15% → NULL\_RATE\_BREACH signal. Root cause: upstream form validation bug introduced in deploy. | DQ checks run on Evidence payload samples (10% for Tier-2, 100% for Tier-1). Reduces compute while catching anomalies. |
| **M8** | **Anomaly Engine v1** | Statistical baselines for volume and freshness (mean, stddev), 2-sigma breach detection, ML feature store for time-series patterns | Static thresholds miss gradual degradation. Anomaly detection catches 'this looks different than usual' even when within hard limits. | Volume usually peaks at 2pm. Today's peak is 20% lower and 2 hours earlier → ANOMALY signal even though absolute volume is above min threshold. | Feature store in S3 \+ DynamoDB. Baseline recomputation runs nightly as batch job, not inline with signal processing. |
| **M9** | **Neptune RCA Enrichment** | Owner, Team, SLO nodes in Neptune, blast radius traversal queries operational (\<500ms p99), 50% of incidents auto-correlated to root cause deployment | Ownership edges enable automated routing: Incident → Dataset → Owner → PagerDuty. Blast radius queries show downstream impact of upstream failures. | Schema change in orders.created impacts 12 downstream consumers. Blast radius query returns ranked list: 'finance-pipeline (CRITICAL), analytics-dashboard (WARNING)'. | Blast radius query depth limited to 3 hops. Results cached for 5min. Typical query: \<500ms for datasets with \<100 consumers. |
| **M10** | **Alerting Maturation** | Severity-based routing (SEV-1→PagerDuty, SEV-2→Slack, SEV-3→Jira), escalation policies, alert grouping by root cause, false positive tracking | Alert fatigue kills adoption. Grouping reduces noise (10 signals from 1 incident \= 1 alert). False positive tracking enables continuous tuning. | 5 datasets fail due to single upstream producer. Alert grouping creates 1 incident with 5 impacted datasets, not 5 separate pages. | Alert dedup window: 5min. Max alerts/hour/team: 10 (configurable). Breakers prevent alert storms during major outages. |
| **M11** | **Anomaly Engine v2** | Seasonal pattern detection (daily/weekly cycles), dynamic thresholds per time-of-day, Cost Engine prototype for compute/storage anomaly detection | Weekend volume is naturally lower. Anomaly Engine v2 learns these patterns to avoid false positives on predictable variations. | Saturday volume is 40% of weekday. V1 would alert. V2 knows Saturday baseline and only alerts if Saturday volume drops below Saturday baseline. | Seasonal models stored per (dataset, day\_of\_week, hour). 168 baseline points per dataset. Recomputed weekly. |
| **M12** | **Signal Platform Maturity** | All 5 engines (Freshness, Volume, Contract, DQ, Anomaly) at scale, Signal SLA: \<5 min from evidence to actionable signal, Knowledge Plane query latency \<2s for RCA | Platform SLAs create accountability. \<5 min signal latency ensures incidents are detected before customers report them. | Producer breaks at 10:00:00. Evidence at 10:00:02. Signal at 10:05:00. Alert at 10:05:15. Oncall engaged before customer impact. | 50K events/sec sustained. 5-min aggregation windows. Neptune queries \<2s p99. DynamoDB queries \<10ms p99. |

# **Team 3: Autopilot — Detailed Milestones**

Team 3 owns the Autopilot system: Push model for proactive baseline instrumentation and Pull model for reactive improvement. The team solves the bootstrap problem—ensuring applications emit signals before failures occur.

| Mo | Milestone | Deliverables | Why (Rationale) | Example | Scale |
| :---: | ----- | ----- | ----- | ----- | ----- |
| **M1** | **Autopilot Infrastructure** | GitHub App registration with repo access, repo scanning framework, AST analysis libraries (tree-sitter for Python/Java/Scala), Scout Agent skeleton | Push model requires programmatic repo access. AST analysis identifies instrumentation points without executing code. Tree-sitter supports 40+ languages. | Scout Agent clones orders-svc repo, parses Python files, finds all KafkaProducer.send() calls, identifies missing OTel context injection. | Parallel repo scanning: 10 repos/minute. AST parsing: \<30s for repos with \<10K files. Results cached to avoid repeated analysis. |
| **M2** | **Baseline Scanner v1** | Kafka producer pattern detection (Python kafka-python, Java KafkaProducer, Scala), language/framework fingerprinting for 80% of repos | Detection patterns must cover real codebases. Python kafka-python and Java KafkaProducer cover 80% of our producer code. Fingerprinting enables framework-specific PR generation. | Detected: Python 3.9, FastAPI, kafka-python. Generated PR will use Python-specific OTel SDK and kafka-python interceptor patterns. | Pattern library: 50 detection rules covering 6 languages. New patterns added via config, not code deployment. |
| **M3** | **Gap Analysis & Prioritization** | Per-repo observability score (0-100), priority backlog using formula: (Tier×3) \+ (Gap×2) \+ (Downstream×1), gap reports published to teams | Scoring creates objective prioritization. Tier-1 services with P0 gaps and many downstream consumers get PRs first. Transparency builds trust. | orders-svc: Score=35 (OTel=0, Contracts=40, Timestamps=100). Priority=85 (Tier-1×3 \+ P0 gap×2 \+ 12 consumers×1). Ranks \#2 in backlog. | Gap analysis runs nightly. Results stored in DynamoDB for dashboard queries. Backlog refreshed weekly. |
| **M4** | **OTel PR Generation** | Automated PRs for REQ-OT-001 (trace propagation) and REQ-OT-003 (producer headers), 10 Tier-1 services receive auto-generated PRs | OTel trace propagation is P0 because RCA causality depends on trace\_id. Producer headers enable HIGH confidence attribution. PRs reduce developer toil. | PR adds: from opentelemetry.propagate import inject; headers={}; inject(headers); producer.send(topic, value, headers=headers) | PR generation: \<5min per repo. LLM-assisted code generation with human-reviewable diffs. Confidence score determines auto-merge vs. manual review. |
| **M5** | **Contract Inference Engine** | Sample-based field detection from 7-day production traffic, contract YAML draft generation, confidence scores per field (required vs. optional), human review workflow | REQ-DC-001 (contract files) requires knowing required fields. Inference from production samples beats manual specification. Human review catches inference errors. | 7 days of orders.created events analyzed. customer\_id present in 100% of samples → inferred required. discount\_code present in 23% → inferred optional. | Sample analysis runs on 10K events per topic (statistically significant). Contract drafts generated in \<1min. Human review SLA: 48h. |
| **M6** | **Push Phase Execution (Week 3-4)** | All Tier-1 services receive PRs for REQ-OT-001, REQ-OT-003, REQ-FR-001 (timestamps), REQ-DC-001 (contract drafts), executive mandate for merge SLA | Push Phase requires organizational commitment. Without executive mandate, PRs languish unreviewed. Merge SLA (2 weeks) creates accountability. | Week 3: 25 PRs created for Tier-1 services. Week 4: 20 merged (80% rate). 5 pending → escalation to service owners. Weekly progress dashboard to leadership. | 25-50 PRs/week during push phase. GitHub App rate limits: 5000 requests/hour sufficient. PR tracking in DynamoDB. |
| **M7** | **Push Phase Execution (Week 5-6)** | Tier-1 \+ Tier-2 coverage extended, PRs for REQ-OT-002 (span creation), REQ-DQ-001 (DQ checks), REQ-DC-002 (producer validation), merge rate \>70% | Tier-2 services complete the critical data paths. Span creation enables precise RCA ('which operation failed'). Producer-side validation catches issues before kafka publish. | PR adds span creation: with tracer.start\_as\_current\_span('process\_order') as span: span.set\_attribute('order\_id', order.id); ... | Tier-2 adds \~50 more services. Total PR volume: 75-100 during weeks 5-6. Parallel review tracks maintain velocity. |
| **M8** | **Push Phase Complete (Week 7-8)** | Full coverage PRs for REQ-VOL-001 (volume metrics), REQ-FR-002 (freshness SLOs), REQ-VOL-002 (volume SLOs), Push→Pull transition criteria validated | Volume SLOs enable anomaly detection. Transition criteria (\>80% OTel, \>80% contracts) ensure Pull model has enough signal quality to be useful. | Contract YAML now includes SLOs: freshness.max\_delay\_seconds: 900, volume.min\_events\_per\_hour: 1000\. Signal Engines can evaluate breaches. | Push phase complete. Maintenance mode: 5-10 PRs/week for new services. CI integration prevents regression. |
| **M9** | **Pull Model Activation** | Evidence Bus failure triggers reactive PRs, closed-loop: Contract FAIL → Autopilot fix suggestion → PR, first 10 reactive PRs merged | Pull model is the steady-state. Evidence failures automatically generate remediation PRs. Closed-loop reduces MTTR for instrumentation gaps. | Contract Engine detects new failure pattern (INVALID\_FORMAT:email). Autopilot analyzes code, generates PR adding email validation. Team merges within SLA. | Reactive PRs: 5-20/week based on failure volume. Dedup ensures same failure doesn't generate multiple PRs. |
| **M10** | **CI Integration** | Pre-merge observability checks in CI pipeline, contract validation in PR checks, merge blocked if OTel coverage drops, Scout Agent confidence scoring tuned | Prevention \> detection. CI checks prevent observability regressions before code merges. Confidence scoring improves PR quality over time. | Developer removes OTel import → CI fails with 'OTel coverage dropped from 85% to 72% (threshold: 80%)'. PR cannot merge until fixed. | CI checks add \<2min to PR pipeline. Observability score computed incrementally from cached AST analysis. |
| **M11** | **Batch Instrumentation Push** | Spark job OpenLineage injection patterns, Airflow callback templates, 50% of Tier-1 batch jobs with automated lineage emission | Batch jobs are 60% of data processing. OpenLineage provides the run\_id correlation needed for batch RCA. Templates accelerate adoption. | Autopilot adds to Spark job: from openlineage.spark import OpenLineageSparkListener; spark.sparkContext.addSparkListener(OpenLineageSparkListener()) | Batch PRs are simpler (fewer integration points). 20-30 Spark jobs instrumented in M11. Remaining covered in M12. |
| **M12** | **Self-Sustaining Flywheel** | Autopilot operational metrics: PR merge rate \>80%, time-to-baseline \<8 weeks for new services, flywheel: evidence failures → instrumentation PRs → better signals | Success \= platform improves itself. New services automatically onboarded. Failures automatically remediated. Platform team focuses on capabilities, not firefighting. | New service checkout-v2 deployed. Within 8 weeks: baseline scan → gap PRs → merged → evidence flowing → signals active. Zero manual intervention. | Steady-state: 10-20 PRs/week. \<1 FTE to maintain Autopilot. Remaining capacity for new detection patterns and capabilities. |

# **Team 4: AI & Intelligence — Detailed Milestones**

Team 4 owns the Consumption Plane: RCA Copilot for AI-powered root cause analysis, SCA Lineage integration for design-time intent, and governance/visualization interfaces. The team delivers the user-facing intelligence layer that reduces MTTR.

| Mo | Milestone | Deliverables | Why (Rationale) | Example | Scale |
| :---: | ----- | ----- | ----- | ----- | ----- |
| **M1** | **RCA Copilot Foundation** | LLM integration (Claude API), prompt engineering for incident context summarization, basic Incident → Dataset → Deployment traversal proof-of-concept | RCA Copilot is the primary value delivery mechanism. Early PoC validates that LLM can reason over Neptune graph \+ Evidence samples to produce useful explanations. | Input: incident\_id. Output: 'Incident caused by orders-svc deployment v3.17 which introduced MISSING\_FIELD:customer\_id at 09:58:02 UTC.' | LLM calls rate-limited: 100/min. Response caching for repeated queries. Fallback to template-based output if LLM unavailable. |
| **M2** | **Deterministic RCA v1** | Neptune traversal: Incident → FailureSignature → Deployment, first\_bad/last\_good timestamp identification, structured output format for incident reports | Deterministic traversal ensures reproducible RCA. Timestamps enable precise 'when did it start?' answers. Structured output integrates with ticketing systems. | RCA output includes: first\_bad\_ts=09:58:02, last\_good\_ts=09:57:59, deployment=orders-svc:v3.17, confidence=HIGH (all edges present). | Neptune query depth limited to 5 hops. Typical RCA query: \<500ms. Results cached by incident\_id for 1 hour. |
| **M3** | **SCA Lineage Interface Spec** | LineageSpec JSON schema finalized with SCA team, URN conventions aligned (dataset URN, column URN), confidence model defined, Lineage Ingestor skeleton deployed | SCA provides design-time intent (what code should read/write). Interface spec ensures clean handoff. Confidence model prevents low-quality lineage from misleading RCA. | LineageSpec includes: producer='orders-delta-landing', inputs=\[urn:dp:orders:order\_created:v1\], outputs=\[urn:dp:orders:order\_created\_curated:v1\], confidence=HIGH | LineageSpec events: \~100/day (per-deployment, not per-record). Ingestor handles 1K specs/day with headroom. |
| **M4** | **Lineage Ingestor v1** | Consume LineageSpec events, write Neptune topology edges (Job → READS/WRITES → Dataset), DynamoDB indexes (DatasetToWritersIndex, DatasetToReadersIndex) | Lineage edges enable blast radius queries. DynamoDB indexes provide O(1) lookups for common queries ('who reads this dataset?'). | Query DatasetToReadersIndex for urn:dp:orders:order\_created:v1 → returns \[{consumer='finance-pipeline', confidence='HIGH'}, {consumer='analytics-dashboard', confidence='MEDIUM'}\] | Lineage edges are bounded (per-deployment, not per-execution). TTL: 90 days for LineageSpec references. Neptune node limit: 10K per dataset. |
| **M5** | **Column-Level Lineage** | Column nodes in Neptune, READS\_COL/WRITES\_COL edges, steel thread: schema drift incident shows implicated columns and impacted consumers | Column-level lineage answers 'which specific field change broke which specific consumers?' This precision dramatically improves RCA usefulness. | Schema drift removes payment\_method column. RCA shows: 3 consumers read payment\_method column → finance-pipeline (HIGH confidence), revenue-dashboard (MEDIUM), audit-export (LOW). | Column cardinality: \~50 columns/dataset average. Column nodes are low-overhead. READS\_COL edges bounded by (jobs × columns) per dataset. |
| **M6** | **RCA Copilot v1 Production** | Sub-2-minute query latency, blast radius ranking by downstream impact, mitigation suggestions (rollback, hotfix, quarantine), first 10 incidents resolved with Copilot-assisted RCA | \<2 min RCA is the North Star metric. Mitigation suggestions reduce cognitive load on oncall. Real incident validation proves value before wider rollout. | Oncall receives page. Opens Copilot. Sees: 'Root cause: deployment v3.17. Blast radius: 3 consumers. Suggested action: rollback to v3.16.' Time from page to action: \<5 min. | Copilot handles 50 concurrent queries. LLM costs: \~$2K/month at 1K queries/day. Caching reduces redundant LLM calls by 60%. |
| **M7** | **Deployment Correlation** | CI/CD event ingestion (DeploymentEvent → commit SHA), LineageSpec ↔ Deployment linkage via commit, RCA shows exact deployed version at incident time | Deployment correlation is critical for 'which deploy caused this?' Commit SHA linkage enables precise version identification across systems. | Incident at 10:05. RCA queries: 'What was deployed version of orders-delta-landing at 10:05?' → v2026.01.16.1 (deployed at 10:00, commit=9f31c2d). | DeploymentEvent volume: \~500/day. Stored in Neptune with 30-90 day TTL. Query by (job\_name, timestamp) returns deployment chain. |
| **M8** | **Consumer Dashboard v1** | Incident timeline visualization, signal history charts, blast radius graph visualization, self-service RCA queries for SRE and data engineers | Dashboard democratizes access. SREs and data engineers can investigate without waiting for Platform team. Visualization aids comprehension. | Dashboard shows: timeline of signal state changes, graph of impacted datasets/consumers, evidence samples with payload diffs. | Dashboard queries SignalState and IncidentIndex (DynamoDB). Neptune queries only for RCA deep-dive. Handles 100 concurrent users. |
| **M9** | **RCA Copilot v2** | Natural language incident summaries, automated Jira ticket enrichment, confidence scoring for RCA assertions (HIGH/MEDIUM/LOW) | Natural language reduces interpretation burden. Jira enrichment ensures investigation context persists. Confidence scoring sets appropriate expectations. | Jira ticket auto-populated: 'Summary: orders-svc v3.17 broke customer\_id field. Impact: 3 downstream pipelines. RCA confidence: HIGH. Evidence IDs: \[evd-123, evd-456\].' | Jira API integration: 100 tickets/day. LLM summarization: \<$1/ticket. Batch enrichment for lower-priority incidents. |
| **M10** | **Proactive Alerting** | Copilot pre-computes blast radius for WARNING signals, early warning notifications to potentially impacted teams before escalation to CRITICAL | Proactive \> reactive. WARNING signals with blast radius enable teams to prepare before incidents. Reduces MTTR by front-loading investigation. | Freshness WARNING on orders.created. Copilot computes: '5 downstream consumers may be impacted if this escalates.' Notifies consumer owners proactively. | Blast radius pre-computation runs on WARNING signal creation. Cached for 5 min. Notification dedup: 1 per consumer per hour. |
| **M11** | **Governance UI** | Contract browsing with version history, lineage explorer with graph visualization, ownership mapping, data catalog integration for business context | Governance UI enables non-incident use cases: understanding data flow, validating contracts, identifying ownership. Catalog integration adds business metadata. | Data analyst explores: 'What transforms order\_created into revenue\_daily?' Lineage explorer shows: order\_created → landing\_job → curated → aggregation\_job → revenue\_daily. | Lineage explorer pre-aggregates paths for common queries. Graph rendering limited to 100 nodes (expand on demand). Catalog sync: nightly batch. |
| **M12** | **RCA at Scale** | RCA Copilot \<2 min MTTR improvement validated across 50+ incidents, 50% of incidents have automated root cause identification, lineage coverage \>90% Tier-1 datasets | Success \= measurable MTTR improvement. 50% automated identification is transformational. 90% lineage coverage ensures blast radius analysis is reliable. | Monthly report: 120 incidents. 65 (54%) had automated root cause. Average time-to-root-cause: 1.8 min (vs. 45 min baseline). MTTR improved from 12h to 2.1h. | Platform handles 500 incidents/month. Lineage refresh cycle: daily for Tier-1, weekly for Tier-2. Copilot improvements via prompt tuning, not architecture changes. |

# **Infrastructure Cost Estimate**

| Component | Monthly Cost | Notes |
| ----- | :---: | ----- |
| EKS (Enforcer \+ Engines) | \~$5,000 | 8-32 instances, autoscaling |
| Amazon Neptune | \~$4,000 | r5.large, HA configuration |
| DynamoDB | \~$3,000 | On-demand, hot tier |
| MSK (Evidence Bus) | \~$4,000 | 64 partitions, 7d retention |
| S3 (Archive) | \~$1,500 | \~50TB/month with tiering |
| Bedrock (LLM for Copilot) | \~$2,000 | Rate-limited, cached |
| CloudWatch, X-Ray, Misc | \~$1,500 | Platform observability |
| **TOTAL ESTIMATED MONTHLY** | **\~$21,000** |  |

*— End of Document —*